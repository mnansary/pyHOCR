{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnansary/pyHOCR/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HETvK7qQ9YT9",
        "colab_type": "code",
        "outputId": "b2746187-d4d1-4302-f76d-7675ec3b308d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soDuMgCc9q8Q",
        "colab_type": "code",
        "outputId": "05e33c09-3398-43ca-973c-2fd7f31c2eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/OCR/pyHOCR/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/OCR/pyHOCR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i948Ic0wXmZ",
        "colab_type": "code",
        "outputId": "aed930d3-4b45-4238-a84f-fa77c454ca15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!pip3 install tensorflow==1.13.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.16.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.15.5)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW2mNC--9z0K",
        "colab_type": "text"
      },
      "source": [
        "# colab specific task\n",
        "\n",
        "```\n",
        "# mount google drive \n",
        "# change working directory to git repo\n",
        "# update repo (if needed)\n",
        "# TPU check\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zmoFYD7-GKK",
        "colab_type": "text"
      },
      "source": [
        "### TPU check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMCASAmw9yhZ",
        "colab_type": "code",
        "outputId": "3d12309f-b769-4fd4-fc2f-38ff2cc11618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.123.182.226:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 13353127054585782585),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3960466896851279111),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10083872595087331646),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6795121186322065703),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 486304870448692684),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 17308647051292267530),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 12042201856240171132),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2814125367760688655),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 16974759967469855232),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5604226538636961019),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 9803633444280625786)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOa6x3xp-0gr",
        "colab_type": "text"
      },
      "source": [
        "# DenseNet Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co8m8wUM-8Vj",
        "colab_type": "text"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyZnl7gp-7jB",
        "colab_type": "code",
        "outputId": "de5a1660-6e05-47bd-a723-8cfa87dfca40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "\"\"\"\n",
        "@author: MD.Nazmuddoha Ansary\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from DenseNet.utils import readh5\n",
        "# dataset path\n",
        "dset_dir=os.path.join(os.getcwd(),'DataSet')\n",
        "# h5 paths\n",
        "Xt_p=os.path.join(dset_dir,'Xt.h5')\n",
        "Yt_p=os.path.join(dset_dir,'Yt.h5')\n",
        "Xv_p=os.path.join(dset_dir,'Xv.h5')\n",
        "Yv_p=os.path.join(dset_dir,'Yv.h5')\n",
        "\n",
        "# train and validation data\n",
        "Xt=readh5(Xt_p)\n",
        "Yt=readh5(Yt_p)\n",
        "Xv=readh5(Xv_p)\n",
        "Yv=readh5(Yv_p)\n",
        "\n",
        "# data set shapes\n",
        "print('X-Train:{}'.format(Xt.shape))\n",
        "print('Y-Train:{}'.format(Yt.shape))\n",
        "print('X-Valid:{}'.format(Xv.shape))\n",
        "print('Y-Valid:{}'.format(Yv.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "X-Train:(9600, 32, 32, 1)\n",
            "Y-Train:(9600, 50)\n",
            "X-Valid:(2400, 32, 32, 1)\n",
            "Y-Valid:(2400, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1UAwYnMAeFr",
        "colab_type": "text"
      },
      "source": [
        "### Build Model\n",
        "\n",
        "\n",
        "*   The Original Keras Model needs to converted to a TF model\n",
        "*   Compile with optimizers and loss function from TF \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZJPJJ_HAc6J",
        "colab_type": "code",
        "outputId": "5a8104df-0386-42d7-d4b2-fbe97ffa1f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from DenseNet.models import denseNet\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model,model_name=denseNet()\n",
        "                  \n",
        "model.summary()\n",
        "print(model_name)\n",
        "# compile\n",
        "model.compile(optimizer=Adam(), loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 24)   216         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 32, 32, 24)   96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 32, 32, 24)   96          batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 24)   0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 12)   2592        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 36)   0           batch_normalization_v1[0][0]     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 32, 32, 36)   144         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 36)   0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 24)   7776        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 60)   0           batch_normalization_v1[0][0]     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 32, 32, 60)   240         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 60)   0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 36)   19440       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 96)   0           batch_normalization_v1[0][0]     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 32, 32, 96)   384         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 96)   0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 48)   41472       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 144)  0           batch_normalization_v1[0][0]     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 32, 32, 144)  576         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 144)  0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 60)   77760       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 204)  0           batch_normalization_v1[0][0]     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 32, 32, 204)  816         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 204)  0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 72)   132192      activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 276)  0           batch_normalization_v1[0][0]     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_7 (Batch (None, 32, 32, 276)  1104        concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 276)  0           batch_normalization_v1_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 84)   208656      activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 360)  0           batch_normalization_v1[0][0]     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_8 (Batch (None, 32, 32, 360)  1440        concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 360)  0           batch_normalization_v1_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 96)   311040      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 456)  0           batch_normalization_v1[0][0]     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_9 (Batch (None, 32, 32, 456)  1824        concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 456)  0           batch_normalization_v1_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 108)  443232      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 564)  0           batch_normalization_v1[0][0]     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_10 (Batc (None, 32, 32, 564)  2256        concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 564)  0           batch_normalization_v1_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 120)  609120      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 684)  0           batch_normalization_v1[0][0]     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_11 (Batc (None, 32, 32, 684)  2736        concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 684)  0           batch_normalization_v1_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 132)  812592      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 816)  0           batch_normalization_v1[0][0]     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_12 (Batc (None, 32, 32, 816)  3264        concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 816)  0           batch_normalization_v1_12[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 144)  1057536     activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 960)  0           batch_normalization_v1[0][0]     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_13 (Batc (None, 32, 32, 960)  3840        concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 960)  0           batch_normalization_v1_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 78)   74880       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 78)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_14 (Batc (None, 16, 16, 78)   312         average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 78)   0           batch_normalization_v1_14[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 78)   54756       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 16, 16, 156)  0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_15 (Batc (None, 16, 16, 156)  624         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 156)  0           batch_normalization_v1_15[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 90)   126360      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 246)  0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_16 (Batc (None, 16, 16, 246)  984         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 246)  0           batch_normalization_v1_16[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 102)  225828      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 348)  0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_17 (Batc (None, 16, 16, 348)  1392        concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 348)  0           batch_normalization_v1_17[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 114)  357048      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 462)  0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_18 (Batc (None, 16, 16, 462)  1848        concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 462)  0           batch_normalization_v1_18[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 126)  523908      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 588)  0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_19 (Batc (None, 16, 16, 588)  2352        concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 588)  0           batch_normalization_v1_19[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 138)  730296      activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 726)  0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_20 (Batc (None, 16, 16, 726)  2904        concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 726)  0           batch_normalization_v1_20[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 150)  980100      activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 876)  0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_21 (Batc (None, 16, 16, 876)  3504        concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 876)  0           batch_normalization_v1_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 162)  1277208     activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 1038) 0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_22 (Batc (None, 16, 16, 1038) 4152        concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 1038) 0           batch_normalization_v1_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 174)  1625508     activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 1212) 0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_23 (Batc (None, 16, 16, 1212) 4848        concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 1212) 0           batch_normalization_v1_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 186)  2028888     activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 1398) 0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_24 (Batc (None, 16, 16, 1398) 5592        concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 1398) 0           batch_normalization_v1_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 198)  2491236     activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 1596) 0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_25 (Batc (None, 16, 16, 1596) 6384        concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 1596) 0           batch_normalization_v1_25[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 210)  3016440     activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 1806) 0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_26 (Batc (None, 16, 16, 1806) 7224        concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 1806) 0           batch_normalization_v1_26[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 111)  200466      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 111)    0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_27 (Batc (None, 8, 8, 111)    444         average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 111)    0           batch_normalization_v1_27[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 111)    110889      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 8, 8, 222)    0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_28 (Batc (None, 8, 8, 222)    888         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 222)    0           batch_normalization_v1_28[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 123)    245754      activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 345)    0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_29 (Batc (None, 8, 8, 345)    1380        concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 345)    0           batch_normalization_v1_29[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 135)    419175      activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 480)    0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_30 (Batc (None, 8, 8, 480)    1920        concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 480)    0           batch_normalization_v1_30[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 147)    635040      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 627)    0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_31 (Batc (None, 8, 8, 627)    2508        concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 627)    0           batch_normalization_v1_31[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 159)    897237      activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 786)    0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_32 (Batc (None, 8, 8, 786)    3144        concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 786)    0           batch_normalization_v1_32[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 171)    1209654     activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 957)    0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_33 (Batc (None, 8, 8, 957)    3828        concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 957)    0           batch_normalization_v1_33[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 183)    1576179     activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 1140)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_34 (Batc (None, 8, 8, 1140)   4560        concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 1140)   0           batch_normalization_v1_34[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 195)    2000700     activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 1335)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_35 (Batc (None, 8, 8, 1335)   5340        concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 1335)   0           batch_normalization_v1_35[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 207)    2487105     activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 1542)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_36 (Batc (None, 8, 8, 1542)   6168        concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 1542)   0           batch_normalization_v1_36[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 219)    3039282     activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 1761)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_37 (Batc (None, 8, 8, 1761)   7044        concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 1761)   0           batch_normalization_v1_37[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 231)    3661119     activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 1992)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_38 (Batc (None, 8, 8, 1992)   7968        concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 1992)   0           batch_normalization_v1_38[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 243)    4356504     activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 2235)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_39 (Batc (None, 8, 8, 2235)   8940        concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 2235)   0           batch_normalization_v1_39[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 2235)         0           activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 50)           111800      global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 38,302,052\n",
            "Trainable params: 38,244,518\n",
            "Non-trainable params: 57,534\n",
            "__________________________________________________________________________________________________\n",
            "DenseNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyB4m0_sChGI",
        "colab_type": "text"
      },
      "source": [
        "### Convert Keras Model to TPU Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWc3-0zmCr_3",
        "colab_type": "code",
        "outputId": "fb39e9e8-1e19-4467-be10-56f03a853deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "model = tf.contrib.tpu.keras_to_tpu_model(model,strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.123.182.226:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 13353127054585782585)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3960466896851279111)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10083872595087331646)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6795121186322065703)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 486304870448692684)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 17308647051292267530)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 12042201856240171132)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2814125367760688655)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 16974759967469855232)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5604226538636961019)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 9803633444280625786)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrwJ5zfNNbLa",
        "colab_type": "text"
      },
      "source": [
        "### Training Parameters and Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvAEJdCkNgys",
        "colab_type": "code",
        "outputId": "1a5b5e97-7df0-436b-f0fd-336a5c0d3695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "c_path=os.path.join(os.getcwd(),'DenseNet','model_weights')\n",
        "import h5py\n",
        "print(c_path)\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(c_path,'{}.h5'.format(model_name)), verbose=1, save_best_only=True)\n",
        "\n",
        "epochs = 150\n",
        "batch_size =30"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybtY3j2KPeBC",
        "colab_type": "text"
      },
      "source": [
        "### Fit the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bh1t_uaPtps",
        "colab_type": "code",
        "outputId": "f4722bcf-574c-44e0-d862-10a5232a4f91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history=model.fit(Xt,Yt,validation_data=(Xv,Yv),epochs=epochs,callbacks=[checkpoint], batch_size=batch_size, verbose=1)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9600 samples, validate on 2400 samples\n",
            "Epoch 1/150\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(3,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(3, 32, 32, 1), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(3, 50), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f24be4b4080> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 58.30991506576538 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.0010000000474974513 {0.001}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 2.6324 - acc: 0.3093INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(3,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(3, 32, 32, 1), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(3, 50), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f24a5f36780> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 20.54944133758545 secs\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 9.98316, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 346s 36ms/sample - loss: 2.6273 - acc: 0.3108 - val_loss: 9.9832 - val_acc: 0.0370\n",
            "Epoch 2/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.9667 - acc: 0.7317\n",
            "Epoch 00002: val_loss improved from 9.98316 to 0.65380, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 61s 6ms/sample - loss: 0.9663 - acc: 0.7318 - val_loss: 0.6538 - val_acc: 0.8250\n",
            "Epoch 3/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.6110 - acc: 0.8328\n",
            "Epoch 00003: val_loss improved from 0.65380 to 0.45209, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 61s 6ms/sample - loss: 0.6105 - acc: 0.8329 - val_loss: 0.4521 - val_acc: 0.8724\n",
            "Epoch 4/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.4499 - acc: 0.8804\n",
            "Epoch 00004: val_loss improved from 0.45209 to 0.34166, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 63s 7ms/sample - loss: 0.4498 - acc: 0.8805 - val_loss: 0.3417 - val_acc: 0.9036\n",
            "Epoch 5/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.9001\n",
            "Epoch 00005: val_loss did not improve from 0.34166\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.3762 - acc: 0.9004 - val_loss: 0.6950 - val_acc: 0.8198\n",
            "Epoch 6/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.9107\n",
            "Epoch 00006: val_loss improved from 0.34166 to 0.30169, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 62s 6ms/sample - loss: 0.3256 - acc: 0.9108 - val_loss: 0.3017 - val_acc: 0.9219\n",
            "Epoch 7/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.9231\n",
            "Epoch 00007: val_loss improved from 0.30169 to 0.21032, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 61s 6ms/sample - loss: 0.2797 - acc: 0.9229 - val_loss: 0.2103 - val_acc: 0.9516\n",
            "Epoch 8/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9312\n",
            "Epoch 00008: val_loss did not improve from 0.21032\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.2602 - acc: 0.9314 - val_loss: 0.4214 - val_acc: 0.8964\n",
            "Epoch 9/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9344\n",
            "Epoch 00009: val_loss did not improve from 0.21032\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.2428 - acc: 0.9345 - val_loss: 0.2453 - val_acc: 0.9391\n",
            "Epoch 10/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9449\n",
            "Epoch 00010: val_loss improved from 0.21032 to 0.20494, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 61s 6ms/sample - loss: 0.2012 - acc: 0.9449 - val_loss: 0.2049 - val_acc: 0.9443\n",
            "Epoch 11/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9459\n",
            "Epoch 00011: val_loss did not improve from 0.20494\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.2069 - acc: 0.9454 - val_loss: 0.3780 - val_acc: 0.9182\n",
            "Epoch 12/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9429\n",
            "Epoch 00012: val_loss did not improve from 0.20494\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.2123 - acc: 0.9430 - val_loss: 0.2985 - val_acc: 0.9292\n",
            "Epoch 13/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9523\n",
            "Epoch 00013: val_loss did not improve from 0.20494\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.1811 - acc: 0.9522 - val_loss: 0.2092 - val_acc: 0.9510\n",
            "Epoch 14/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9549\n",
            "Epoch 00014: val_loss did not improve from 0.20494\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.1773 - acc: 0.9548 - val_loss: 0.2517 - val_acc: 0.9385\n",
            "Epoch 15/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9509\n",
            "Epoch 00015: val_loss did not improve from 0.20494\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.1939 - acc: 0.9505 - val_loss: 0.2297 - val_acc: 0.9495\n",
            "Epoch 16/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9623\n",
            "Epoch 00016: val_loss did not improve from 0.20494\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.1384 - acc: 0.9620 - val_loss: 0.3117 - val_acc: 0.9307\n",
            "Epoch 17/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.9594\n",
            "Epoch 00017: val_loss did not improve from 0.20494\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.1634 - acc: 0.9594 - val_loss: 0.2279 - val_acc: 0.9531\n",
            "Epoch 18/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9612\n",
            "Epoch 00018: val_loss did not improve from 0.20494\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.1432 - acc: 0.9611 - val_loss: 0.2055 - val_acc: 0.9594\n",
            "Epoch 19/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9693\n",
            "Epoch 00019: val_loss improved from 0.20494 to 0.18497, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 63s 7ms/sample - loss: 0.1285 - acc: 0.9694 - val_loss: 0.1850 - val_acc: 0.9557\n",
            "Epoch 20/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9662\n",
            "Epoch 00020: val_loss did not improve from 0.18497\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.1297 - acc: 0.9663 - val_loss: 0.2211 - val_acc: 0.9563\n",
            "Epoch 21/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9698\n",
            "Epoch 00021: val_loss improved from 0.18497 to 0.15488, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 64s 7ms/sample - loss: 0.1254 - acc: 0.9699 - val_loss: 0.1549 - val_acc: 0.9635\n",
            "Epoch 22/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9697\n",
            "Epoch 00022: val_loss did not improve from 0.15488\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.1240 - acc: 0.9698 - val_loss: 0.1948 - val_acc: 0.9594\n",
            "Epoch 23/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9745\n",
            "Epoch 00023: val_loss did not improve from 0.15488\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.1088 - acc: 0.9745 - val_loss: 0.2531 - val_acc: 0.9500\n",
            "Epoch 24/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9698\n",
            "Epoch 00024: val_loss did not improve from 0.15488\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.1244 - acc: 0.9699 - val_loss: 0.3420 - val_acc: 0.9375\n",
            "Epoch 25/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9768\n",
            "Epoch 00025: val_loss did not improve from 0.15488\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.0970 - acc: 0.9767 - val_loss: 0.2753 - val_acc: 0.9411\n",
            "Epoch 26/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9757\n",
            "Epoch 00026: val_loss did not improve from 0.15488\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.1028 - acc: 0.9758 - val_loss: 0.2061 - val_acc: 0.9604\n",
            "Epoch 27/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9792\n",
            "Epoch 00027: val_loss did not improve from 0.15488\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0913 - acc: 0.9790 - val_loss: 0.2315 - val_acc: 0.9495\n",
            "Epoch 28/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9731\n",
            "Epoch 00028: val_loss did not improve from 0.15488\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.1116 - acc: 0.9729 - val_loss: 0.1812 - val_acc: 0.9615\n",
            "Epoch 29/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9726\n",
            "Epoch 00029: val_loss did not improve from 0.15488\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.1122 - acc: 0.9727 - val_loss: 0.1590 - val_acc: 0.9688\n",
            "Epoch 30/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9800\n",
            "Epoch 00030: val_loss improved from 0.15488 to 0.15116, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 63s 7ms/sample - loss: 0.0843 - acc: 0.9801 - val_loss: 0.1512 - val_acc: 0.9682\n",
            "Epoch 31/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9809\n",
            "Epoch 00031: val_loss did not improve from 0.15116\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0898 - acc: 0.9809 - val_loss: 0.2516 - val_acc: 0.9578\n",
            "Epoch 32/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9804\n",
            "Epoch 00032: val_loss did not improve from 0.15116\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0876 - acc: 0.9805 - val_loss: 0.1687 - val_acc: 0.9661\n",
            "Epoch 33/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9820\n",
            "Epoch 00033: val_loss did not improve from 0.15116\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0747 - acc: 0.9819 - val_loss: 0.1915 - val_acc: 0.9589\n",
            "Epoch 34/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9835\n",
            "Epoch 00034: val_loss did not improve from 0.15116\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0711 - acc: 0.9835 - val_loss: 0.1846 - val_acc: 0.9672\n",
            "Epoch 35/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9801\n",
            "Epoch 00035: val_loss improved from 0.15116 to 0.12522, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 65s 7ms/sample - loss: 0.0812 - acc: 0.9799 - val_loss: 0.1252 - val_acc: 0.9714\n",
            "Epoch 36/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9811\n",
            "Epoch 00036: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0823 - acc: 0.9811 - val_loss: 0.1666 - val_acc: 0.9672\n",
            "Epoch 37/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9820\n",
            "Epoch 00037: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0765 - acc: 0.9820 - val_loss: 0.2004 - val_acc: 0.9552\n",
            "Epoch 38/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9812\n",
            "Epoch 00038: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0819 - acc: 0.9813 - val_loss: 0.1531 - val_acc: 0.9693\n",
            "Epoch 39/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9841\n",
            "Epoch 00039: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0752 - acc: 0.9841 - val_loss: 0.2098 - val_acc: 0.9557\n",
            "Epoch 40/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9903\n",
            "Epoch 00040: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.0493 - acc: 0.9904 - val_loss: 0.1365 - val_acc: 0.9740\n",
            "Epoch 41/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9916\n",
            "Epoch 00041: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0467 - acc: 0.9917 - val_loss: 0.1611 - val_acc: 0.9646\n",
            "Epoch 42/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9816\n",
            "Epoch 00042: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0732 - acc: 0.9816 - val_loss: 0.2009 - val_acc: 0.9656\n",
            "Epoch 43/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9888\n",
            "Epoch 00043: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0565 - acc: 0.9888 - val_loss: 0.2319 - val_acc: 0.9500\n",
            "Epoch 44/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9898\n",
            "Epoch 00044: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0495 - acc: 0.9898 - val_loss: 0.3030 - val_acc: 0.9354\n",
            "Epoch 45/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9847\n",
            "Epoch 00045: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0698 - acc: 0.9848 - val_loss: 0.1739 - val_acc: 0.9661\n",
            "Epoch 46/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9875\n",
            "Epoch 00046: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0585 - acc: 0.9875 - val_loss: 0.1729 - val_acc: 0.9661\n",
            "Epoch 47/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9889\n",
            "Epoch 00047: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0582 - acc: 0.9888 - val_loss: 0.1530 - val_acc: 0.9719\n",
            "Epoch 48/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9902\n",
            "Epoch 00048: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0471 - acc: 0.9902 - val_loss: 0.1652 - val_acc: 0.9719\n",
            "Epoch 49/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9873\n",
            "Epoch 00049: val_loss did not improve from 0.12522\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0606 - acc: 0.9874 - val_loss: 0.1461 - val_acc: 0.9682\n",
            "Epoch 50/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9932\n",
            "Epoch 00050: val_loss improved from 0.12522 to 0.11765, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 65s 7ms/sample - loss: 0.0417 - acc: 0.9932 - val_loss: 0.1177 - val_acc: 0.9724\n",
            "Epoch 51/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9907\n",
            "Epoch 00051: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0456 - acc: 0.9906 - val_loss: 0.1371 - val_acc: 0.9641\n",
            "Epoch 52/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9888\n",
            "Epoch 00052: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0518 - acc: 0.9888 - val_loss: 0.1367 - val_acc: 0.9708\n",
            "Epoch 53/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9897\n",
            "Epoch 00053: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0467 - acc: 0.9897 - val_loss: 0.1365 - val_acc: 0.9724\n",
            "Epoch 54/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9894\n",
            "Epoch 00054: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0462 - acc: 0.9895 - val_loss: 0.1369 - val_acc: 0.9719\n",
            "Epoch 55/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9901\n",
            "Epoch 00055: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0490 - acc: 0.9901 - val_loss: 0.1350 - val_acc: 0.9708\n",
            "Epoch 56/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9905\n",
            "Epoch 00056: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0479 - acc: 0.9905 - val_loss: 0.1474 - val_acc: 0.9693\n",
            "Epoch 57/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9924\n",
            "Epoch 00057: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0419 - acc: 0.9924 - val_loss: 0.1579 - val_acc: 0.9688\n",
            "Epoch 58/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9945\n",
            "Epoch 00058: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0334 - acc: 0.9944 - val_loss: 0.1406 - val_acc: 0.9698\n",
            "Epoch 59/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9903\n",
            "Epoch 00059: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.0493 - acc: 0.9902 - val_loss: 0.1680 - val_acc: 0.9661\n",
            "Epoch 60/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9950\n",
            "Epoch 00060: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0330 - acc: 0.9949 - val_loss: 0.1876 - val_acc: 0.9573\n",
            "Epoch 61/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9958\n",
            "Epoch 00061: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0296 - acc: 0.9958 - val_loss: 0.1246 - val_acc: 0.9724\n",
            "Epoch 62/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9944\n",
            "Epoch 00062: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0282 - acc: 0.9943 - val_loss: 0.1623 - val_acc: 0.9661\n",
            "Epoch 63/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9894\n",
            "Epoch 00063: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0465 - acc: 0.9895 - val_loss: 0.1592 - val_acc: 0.9672\n",
            "Epoch 64/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9949\n",
            "Epoch 00064: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0303 - acc: 0.9949 - val_loss: 0.1427 - val_acc: 0.9698\n",
            "Epoch 65/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9931\n",
            "Epoch 00065: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0372 - acc: 0.9931 - val_loss: 0.1706 - val_acc: 0.9682\n",
            "Epoch 66/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9932\n",
            "Epoch 00066: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.0342 - acc: 0.9932 - val_loss: 0.1279 - val_acc: 0.9703\n",
            "Epoch 67/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9961\n",
            "Epoch 00067: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0263 - acc: 0.9961 - val_loss: 0.1496 - val_acc: 0.9688\n",
            "Epoch 68/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9941\n",
            "Epoch 00068: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0323 - acc: 0.9941 - val_loss: 0.1941 - val_acc: 0.9594\n",
            "Epoch 69/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9945\n",
            "Epoch 00069: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0307 - acc: 0.9945 - val_loss: 0.1783 - val_acc: 0.9672\n",
            "Epoch 70/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9903\n",
            "Epoch 00070: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0454 - acc: 0.9904 - val_loss: 0.1385 - val_acc: 0.9708\n",
            "Epoch 71/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9950\n",
            "Epoch 00071: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.0274 - acc: 0.9951 - val_loss: 0.1333 - val_acc: 0.9708\n",
            "Epoch 72/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9949\n",
            "Epoch 00072: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.0317 - acc: 0.9949 - val_loss: 0.2583 - val_acc: 0.9521\n",
            "Epoch 73/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9940\n",
            "Epoch 00073: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.0361 - acc: 0.9940 - val_loss: 0.1341 - val_acc: 0.9740\n",
            "Epoch 74/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9969\n",
            "Epoch 00074: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.0199 - acc: 0.9969 - val_loss: 0.1555 - val_acc: 0.9708\n",
            "Epoch 75/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9979\n",
            "Epoch 00075: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0165 - acc: 0.9979 - val_loss: 0.1218 - val_acc: 0.9745\n",
            "Epoch 76/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9926\n",
            "Epoch 00076: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.0336 - acc: 0.9924 - val_loss: 0.2038 - val_acc: 0.9609\n",
            "Epoch 77/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9943\n",
            "Epoch 00077: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.0313 - acc: 0.9943 - val_loss: 0.1540 - val_acc: 0.9708\n",
            "Epoch 78/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9932\n",
            "Epoch 00078: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0373 - acc: 0.9932 - val_loss: 0.1820 - val_acc: 0.9620\n",
            "Epoch 79/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9943\n",
            "Epoch 00079: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.0315 - acc: 0.9943 - val_loss: 0.1294 - val_acc: 0.9703\n",
            "Epoch 80/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9957\n",
            "Epoch 00080: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0263 - acc: 0.9957 - val_loss: 0.1699 - val_acc: 0.9620\n",
            "Epoch 81/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9952\n",
            "Epoch 00081: val_loss did not improve from 0.11765\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0258 - acc: 0.9952 - val_loss: 0.1208 - val_acc: 0.9776\n",
            "Epoch 82/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9971\n",
            "Epoch 00082: val_loss improved from 0.11765 to 0.10832, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 63s 7ms/sample - loss: 0.0215 - acc: 0.9971 - val_loss: 0.1083 - val_acc: 0.9766\n",
            "Epoch 83/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9978\n",
            "Epoch 00083: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0176 - acc: 0.9978 - val_loss: 0.1168 - val_acc: 0.9745\n",
            "Epoch 84/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9971\n",
            "Epoch 00084: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0189 - acc: 0.9970 - val_loss: 0.1179 - val_acc: 0.9766\n",
            "Epoch 85/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9960\n",
            "Epoch 00085: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0227 - acc: 0.9960 - val_loss: 0.1349 - val_acc: 0.9771\n",
            "Epoch 86/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9976\n",
            "Epoch 00086: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0141 - acc: 0.9977 - val_loss: 0.1268 - val_acc: 0.9771\n",
            "Epoch 87/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9962\n",
            "Epoch 00087: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0199 - acc: 0.9962 - val_loss: 0.2621 - val_acc: 0.9531\n",
            "Epoch 88/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9946\n",
            "Epoch 00088: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0312 - acc: 0.9947 - val_loss: 0.1315 - val_acc: 0.9734\n",
            "Epoch 89/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9963\n",
            "Epoch 00089: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0203 - acc: 0.9964 - val_loss: 0.1216 - val_acc: 0.9760\n",
            "Epoch 90/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9966\n",
            "Epoch 00090: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 23s 2ms/sample - loss: 0.0187 - acc: 0.9966 - val_loss: 0.1258 - val_acc: 0.9734\n",
            "Epoch 91/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9993\n",
            "Epoch 00091: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0103 - acc: 0.9993 - val_loss: 0.1304 - val_acc: 0.9755\n",
            "Epoch 92/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9936\n",
            "Epoch 00092: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0285 - acc: 0.9936 - val_loss: 0.1639 - val_acc: 0.9677\n",
            "Epoch 93/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9979\n",
            "Epoch 00093: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0155 - acc: 0.9979 - val_loss: 0.1367 - val_acc: 0.9698\n",
            "Epoch 94/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9966\n",
            "Epoch 00094: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0213 - acc: 0.9966 - val_loss: 0.1169 - val_acc: 0.9755\n",
            "Epoch 95/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9974\n",
            "Epoch 00095: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0154 - acc: 0.9974 - val_loss: 0.1123 - val_acc: 0.9786\n",
            "Epoch 96/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9966\n",
            "Epoch 00096: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0197 - acc: 0.9966 - val_loss: 0.1324 - val_acc: 0.9750\n",
            "Epoch 97/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9948\n",
            "Epoch 00097: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0245 - acc: 0.9948 - val_loss: 0.1394 - val_acc: 0.9734\n",
            "Epoch 98/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9996\n",
            "Epoch 00098: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0103 - acc: 0.9996 - val_loss: 0.1189 - val_acc: 0.9740\n",
            "Epoch 99/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9954\n",
            "Epoch 00099: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0213 - acc: 0.9953 - val_loss: 0.4133 - val_acc: 0.9151\n",
            "Epoch 100/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9926\n",
            "Epoch 00100: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0316 - acc: 0.9926 - val_loss: 0.1402 - val_acc: 0.9745\n",
            "Epoch 101/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9987\n",
            "Epoch 00101: val_loss did not improve from 0.10832\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0128 - acc: 0.9986 - val_loss: 0.2031 - val_acc: 0.9583\n",
            "Epoch 102/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9979\n",
            "Epoch 00102: val_loss improved from 0.10832 to 0.10629, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 62s 7ms/sample - loss: 0.0164 - acc: 0.9979 - val_loss: 0.1063 - val_acc: 0.9776\n",
            "Epoch 103/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9982\n",
            "Epoch 00103: val_loss did not improve from 0.10629\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0138 - acc: 0.9982 - val_loss: 0.1097 - val_acc: 0.9792\n",
            "Epoch 104/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9997\n",
            "Epoch 00104: val_loss did not improve from 0.10629\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0071 - acc: 0.9997 - val_loss: 0.1170 - val_acc: 0.9750\n",
            "Epoch 105/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9953\n",
            "Epoch 00105: val_loss did not improve from 0.10629\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0211 - acc: 0.9953 - val_loss: 0.1621 - val_acc: 0.9672\n",
            "Epoch 106/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9961\n",
            "Epoch 00106: val_loss did not improve from 0.10629\n",
            "9600/9600 [==============================] - 24s 3ms/sample - loss: 0.0171 - acc: 0.9961 - val_loss: 0.1966 - val_acc: 0.9594\n",
            "Epoch 107/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9961\n",
            "Epoch 00107: val_loss did not improve from 0.10629\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0231 - acc: 0.9961 - val_loss: 0.1158 - val_acc: 0.9760\n",
            "Epoch 108/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9965\n",
            "Epoch 00108: val_loss did not improve from 0.10629\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0166 - acc: 0.9965 - val_loss: 0.2742 - val_acc: 0.9474\n",
            "Epoch 109/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9970\n",
            "Epoch 00109: val_loss did not improve from 0.10629\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0195 - acc: 0.9970 - val_loss: 0.1126 - val_acc: 0.9740\n",
            "Epoch 110/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9990\n",
            "Epoch 00110: val_loss did not improve from 0.10629\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0103 - acc: 0.9990 - val_loss: 0.1460 - val_acc: 0.9693\n",
            "Epoch 111/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9978\n",
            "Epoch 00111: val_loss did not improve from 0.10629\n",
            "9600/9600 [==============================] - 24s 3ms/sample - loss: 0.0131 - acc: 0.9978 - val_loss: 0.2865 - val_acc: 0.9469\n",
            "Epoch 112/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9943\n",
            "Epoch 00112: val_loss did not improve from 0.10629\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0293 - acc: 0.9941 - val_loss: 0.1893 - val_acc: 0.9615\n",
            "Epoch 113/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9965\n",
            "Epoch 00113: val_loss did not improve from 0.10629\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0185 - acc: 0.9965 - val_loss: 0.1205 - val_acc: 0.9734\n",
            "Epoch 114/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9986\n",
            "Epoch 00114: val_loss improved from 0.10629 to 0.10131, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 64s 7ms/sample - loss: 0.0128 - acc: 0.9986 - val_loss: 0.1013 - val_acc: 0.9786\n",
            "Epoch 115/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9979\n",
            "Epoch 00115: val_loss did not improve from 0.10131\n",
            "9600/9600 [==============================] - 24s 3ms/sample - loss: 0.0137 - acc: 0.9979 - val_loss: 0.1094 - val_acc: 0.9760\n",
            "Epoch 116/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9979\n",
            "Epoch 00116: val_loss did not improve from 0.10131\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0159 - acc: 0.9979 - val_loss: 0.1065 - val_acc: 0.9792\n",
            "Epoch 117/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9987\n",
            "Epoch 00117: val_loss did not improve from 0.10131\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0100 - acc: 0.9987 - val_loss: 0.1249 - val_acc: 0.9760\n",
            "Epoch 118/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9982\n",
            "Epoch 00118: val_loss did not improve from 0.10131\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0117 - acc: 0.9980 - val_loss: 0.1355 - val_acc: 0.9693\n",
            "Epoch 119/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9980\n",
            "Epoch 00119: val_loss did not improve from 0.10131\n",
            "9600/9600 [==============================] - 24s 3ms/sample - loss: 0.0142 - acc: 0.9980 - val_loss: 0.1280 - val_acc: 0.9714\n",
            "Epoch 120/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9993\n",
            "Epoch 00120: val_loss improved from 0.10131 to 0.09521, saving model to /content/gdrive/My Drive/OCR/pyHOCR/DenseNet/model_weights/DenseNet.h5\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "9600/9600 [==============================] - 63s 7ms/sample - loss: 0.0075 - acc: 0.9993 - val_loss: 0.0952 - val_acc: 0.9802\n",
            "Epoch 121/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9974\n",
            "Epoch 00121: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0127 - acc: 0.9974 - val_loss: 0.1944 - val_acc: 0.9589\n",
            "Epoch 122/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9957\n",
            "Epoch 00122: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0236 - acc: 0.9957 - val_loss: 0.1392 - val_acc: 0.9714\n",
            "Epoch 123/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9967\n",
            "Epoch 00123: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0179 - acc: 0.9967 - val_loss: 0.1214 - val_acc: 0.9750\n",
            "Epoch 124/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9991\n",
            "Epoch 00124: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0091 - acc: 0.9991 - val_loss: 0.1299 - val_acc: 0.9745\n",
            "Epoch 125/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9982\n",
            "Epoch 00125: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0133 - acc: 0.9982 - val_loss: 0.1234 - val_acc: 0.9792\n",
            "Epoch 126/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9978\n",
            "Epoch 00126: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0120 - acc: 0.9978 - val_loss: 0.1440 - val_acc: 0.9724\n",
            "Epoch 127/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9983\n",
            "Epoch 00127: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0103 - acc: 0.9983 - val_loss: 0.1093 - val_acc: 0.9755\n",
            "Epoch 128/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9986\n",
            "Epoch 00128: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0095 - acc: 0.9986 - val_loss: 0.1461 - val_acc: 0.9724\n",
            "Epoch 129/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9982\n",
            "Epoch 00129: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 3ms/sample - loss: 0.0115 - acc: 0.9982 - val_loss: 0.1515 - val_acc: 0.9714\n",
            "Epoch 130/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9965\n",
            "Epoch 00130: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0171 - acc: 0.9965 - val_loss: 0.1431 - val_acc: 0.9719\n",
            "Epoch 131/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9988\n",
            "Epoch 00131: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0088 - acc: 0.9988 - val_loss: 0.1221 - val_acc: 0.9792\n",
            "Epoch 132/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9965\n",
            "Epoch 00132: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0200 - acc: 0.9965 - val_loss: 0.1823 - val_acc: 0.9651\n",
            "Epoch 133/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9979\n",
            "Epoch 00133: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0125 - acc: 0.9979 - val_loss: 0.1275 - val_acc: 0.9755\n",
            "Epoch 134/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9958\n",
            "Epoch 00134: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0229 - acc: 0.9958 - val_loss: 0.1454 - val_acc: 0.9703\n",
            "Epoch 135/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9992\n",
            "Epoch 00135: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0096 - acc: 0.9992 - val_loss: 0.1157 - val_acc: 0.9786\n",
            "Epoch 136/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9988\n",
            "Epoch 00136: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0091 - acc: 0.9988 - val_loss: 0.1370 - val_acc: 0.9740\n",
            "Epoch 137/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9986\n",
            "Epoch 00137: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0107 - acc: 0.9986 - val_loss: 0.1161 - val_acc: 0.9781\n",
            "Epoch 138/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9975\n",
            "Epoch 00138: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0154 - acc: 0.9975 - val_loss: 0.1219 - val_acc: 0.9776\n",
            "Epoch 139/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9991\n",
            "Epoch 00139: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0084 - acc: 0.9991 - val_loss: 0.1196 - val_acc: 0.9781\n",
            "Epoch 140/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9979\n",
            "Epoch 00140: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0123 - acc: 0.9979 - val_loss: 0.2052 - val_acc: 0.9625\n",
            "Epoch 141/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9984\n",
            "Epoch 00141: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0113 - acc: 0.9984 - val_loss: 0.1264 - val_acc: 0.9771\n",
            "Epoch 142/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9990\n",
            "Epoch 00142: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0094 - acc: 0.9990 - val_loss: 0.1131 - val_acc: 0.9786\n",
            "Epoch 143/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9988\n",
            "Epoch 00143: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0110 - acc: 0.9988 - val_loss: 0.2395 - val_acc: 0.9568\n",
            "Epoch 144/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9980\n",
            "Epoch 00144: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0115 - acc: 0.9980 - val_loss: 0.1493 - val_acc: 0.9682\n",
            "Epoch 145/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9976\n",
            "Epoch 00145: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0131 - acc: 0.9977 - val_loss: 0.1765 - val_acc: 0.9604\n",
            "Epoch 146/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9967\n",
            "Epoch 00146: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 3ms/sample - loss: 0.0167 - acc: 0.9967 - val_loss: 0.1325 - val_acc: 0.9750\n",
            "Epoch 147/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9986\n",
            "Epoch 00147: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0110 - acc: 0.9986 - val_loss: 0.1637 - val_acc: 0.9703\n",
            "Epoch 148/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9975\n",
            "Epoch 00148: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0130 - acc: 0.9975 - val_loss: 0.1310 - val_acc: 0.9740\n",
            "Epoch 149/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9980\n",
            "Epoch 00149: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 3ms/sample - loss: 0.0113 - acc: 0.9980 - val_loss: 0.1276 - val_acc: 0.9698\n",
            "Epoch 150/150\n",
            "9570/9600 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9993\n",
            "Epoch 00150: val_loss did not improve from 0.09521\n",
            "9600/9600 [==============================] - 24s 2ms/sample - loss: 0.0072 - acc: 0.9993 - val_loss: 0.1181 - val_acc: 0.9729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqpZwYCra3MG",
        "colab_type": "text"
      },
      "source": [
        "### Load Best Model For testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6QJ85Iad-5i",
        "colab_type": "text"
      },
      "source": [
        "# Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGHwAwDYa2UT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ffb3b4fd-cce4-4f3a-facb-f40720ee4106"
      },
      "source": [
        "from sklearn import metrics\n",
        "from keras.models import load_model\n",
        "from termcolor import colored\n",
        "\n",
        "# load model with best val_loss\n",
        "model,model_name=denseNet()\n",
        "model.load_weights(os.path.join(c_path,'{}.h5'.format(model_name)))\n",
        "\n",
        "# load tesing data\n",
        "# h5 paths\n",
        "Xtt_p=os.path.join(dset_dir,'Xtt.h5')\n",
        "Ytt_p=os.path.join(dset_dir,'Ytt.h5')\n",
        "\n",
        "# train and validation data\n",
        "Xtt=readh5(Xtt_p)\n",
        "Ytt=readh5(Ytt_p)\n",
        "\n",
        "# data set shapes\n",
        "print('X-Test:{}'.format(Xtt.shape))\n",
        "print('Y-Test:{}'.format(Ytt.shape))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X-Test:(3000, 32, 32, 1)\n",
            "Y-Test:(3000, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xuGXeiNd42-",
        "colab_type": "text"
      },
      "source": [
        "### Get predictions and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9QT0J8wd4Lw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d15d66e3-8d73-4ac6-bc63-857f84f5f304"
      },
      "source": [
        "print(colored('# Generating Predictions','blue'))\n",
        "predictions = [np.argmax(model.predict(np.expand_dims(tensor,axis=0))) for tensor in Xtt]\n",
        "\n",
        "print(colored('# Getting Ground Truth','blue'))\t    \n",
        "ground_truth = [np.argmax(truth_value) for truth_value in Ytt]\n",
        "\n",
        "print(colored('# Calculating Accuracy','blue'))\t    \n",
        "\n",
        "prediction_accuracy = 100* metrics.f1_score(ground_truth,predictions, average = 'micro')\t   \n",
        "print(colored('Test data Prediction Accuracy [F1 accuracy]: {}'.format(prediction_accuracy),'green'))\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m# Generating Predictions\u001b[0m\n",
            "\u001b[34m# Getting Ground Truth\u001b[0m\n",
            "\u001b[34m# Calculating Accuracy\u001b[0m\n",
            "\u001b[32mTest data Prediction Accuracy [F1 accuracy]: 98.36666666666667\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}